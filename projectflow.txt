Create new repo on git and clone it in local.
Create your template.py file and run it.
Add requirements.txt (listed modules taken from yolo V7 git repo)
Add code to setup.py file
Now setup your venv:
    *conda create -n safety python=3.8 -y
    *conda activate safety
    *pip install -r requirements.txt
Now we will setup our logging and exception:
    *Add code to isd.logger.__init__.py
    *Add code to isd.exception.__init__.py
    *Also add code to isd.utils.main_utils.py
Flowchart, Notebook added
Now we will perform Notebook experiment on Google Colab:
    * Add the zipped "isd_data_mini" data file and "YOLOv7_on_Custom_Data.ipynb" 
      notebook to your gdrive and open the notebook.
    * From Colab notebook screen > runtime > change runtime type > T4 GPU > save
      > click 'connect' on right top corner
    * Start executing cells one by one as per the commented instructions.
Now we move to Data Ingestion component:
    * First we need to setup AWS cli in local > Google: AWS cli download > goto first link
      > choose OS > download the .msi file > simple install process > restart IDE/system
    * Go to AWS console.
    * First, create an IAM user(name: yolov7) and download access key csv
    * We need to configure this access key in local bash terminal > aws configure
      > input access&secret key, region and output format(hit enter)
    * Now we will create one S3 bucket > name: mlops-isd-data > uncheck: block access
      > check acknowledge > create bucket
    * Open bucket and upload the data (zipped file)
    * Now we need to write some code to get this data from S3 bucket.
      > work on isd.configuration.s3_operations.py
    * Finally we start the work on "Data Ingestion" component as per our workflow
      > we add constants to isd.constants.training_pipeline.__init__.py
      > Update the isd.entity.config_entity.py
      > Update the isd.entity.artifacts_entity.py
      > Now we update component: "data_ingestion.py"
      > Update the pipeline
      > Update app.py
      > Add "artifacts/*" to .gitignore file

Now, following our workflow, we work on the Data Validation component to make sure we
  have all the required dir and files inside "feature_store" dir after the zip operation.

Before moving to model_trainer component, we need to download yolov7 project from its git repo. (One time task)
  > Execute on terminal: "git clone https://github.com/WongKinYiu/yolov7" This step might through error
    sometimes due to network issue on local or Github side.
  > Create a copy of yolov7.data.coco.yaml and name is as (custom.yaml)
  > Edit your custom.yaml as per the code given in notebook directory
  > Create a copy of yolov7.cfg.training.yolov7.yaml and name is as (custom_yolov7.yaml)
  > Edit your custom_yolov7.yaml (nc: 5)

Next is to work on the component - "model_trainer" as per workflow but execute the 
  "python app.py" from bash terminal this time. Once completed, you will get the best model as 
  artifacts.timestamp.model_trainer.best.pt file. Also, remove .pt from .gitignore file(if exists) 
  and add "yolov7.pt" before pushing it to git.





Workflow:
- constants
- config_entity
- artifact_entity
- components
- pipeline
- app.py
