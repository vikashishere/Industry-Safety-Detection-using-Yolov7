Create new repo on git and clone it in local.
Create your template.py file and run it.
Add requirements.txt (listed modules taken from yolo V7 git repo)
Add code to setup.py file
Now setup your venv:
    *conda create -n safety python=3.8 -y
    *conda activate safety
    *pip install -r requirements.txt
Now we will setup our logging and exception:
    *Add code to isd.logger.__init__.py
    *Add code to isd.exception.__init__.py
    *Also add code to isd.utils.main_utils.py
Flowchart, Notebook added
Now we will perform Notebook experiment on Google Colab:
    * Add the zipped "isd_data_mini" data file and "YOLOv7_on_Custom_Data.ipynb" 
      notebook to your gdrive and open the notebook.
    * From Colab notebook screen > runtime > change runtime type > T4 GPU > save
      > click 'connect' on right top corner
    * Start executing cells one by one as per the commented instructions.
Now we move to Data Ingestion component:
    * First we need to setup AWS cli in local > Google: AWS cli download > goto first link
      > choose OS > download the .msi file > simple install process > restart IDE/system
    * Go to AWS console.
    * First, create an IAM user(name: yolov7) and download access key csv
      Also, instead of full AdministratorAccess to user, we can also set it up for:
      1. AmazonEC2ContainerRegistryFullAccess
      2. AmazonEC2FullAccess
    * We need to configure this access key in local bash terminal > aws configure
      > input access&secret key, region and output format(hit enter)
    * Now we will create one S3 bucket > name: mlops-isd-data > uncheck: block access
      > check acknowledge > create bucket
    * Open bucket and upload the data (zipped file)
    * Now we need to write some code to get this data from S3 bucket.
      > work on isd.configuration.s3_operations.py
    * Finally we start the work on "Data Ingestion" component as per our workflow
      > we add constants to isd.constants.training_pipeline.__init__.py
      > Update the isd.entity.config_entity.py
      > Update the isd.entity.artifacts_entity.py
      > Now we update component: "data_ingestion.py"
      > Update the pipeline
      > Update app.py
      > Add "artifacts/*" to .gitignore file

Now, following our workflow, we work on the Data Validation component to make sure we
  have all the required dir and files inside "feature_store" dir after the zip operation.

Before moving to model_trainer component, we need to download yolov7 project from its git repo. (One time task)
  > Execute on terminal: "git clone https://github.com/WongKinYiu/yolov7" This step might through error
    sometimes due to network issue on local or Github side.
  > Create a copy of yolov7.data.coco.yaml and name is as (custom.yaml)
  > Edit your custom.yaml as per the code given in notebook directory
  > Create a copy of yolov7.cfg.training.yolov7.yaml and name is as (custom_yolov7.yaml)
  > Edit your custom_yolov7.yaml (nc: 5)

Next is to work on the component - "model_trainer" as per workflow but execute the 
  "python app.py" from bash terminal this time. Once completed, you will get the best model as 
  artifacts.timestamp.model_trainer.best.pt file. Also, remove .pt from .gitignore file(if exists) 
  and add "yolov7.pt" before pushing it to git.

Next is to work on component: "model_pusher" as per our workflow.

We move to work on creating our user app for prediction:
  > Add template.index.html (title on line 200)
  > Now add code to app.py (as per line 51, your best.pt model should be there within yolov7 dir)
  > Run your "python app.py" from bash terminal (powershell might do issue)

We will next work on the CICD:
  > Setup your .github.worflows.cicd.yaml file
  > Setup your dockerfile
  > Add items to .dockerignore file (e.g. flowchart, notebook, env and log etc)
  > We will also need one ECR repo -> Region: us-east-1 -> Create -> private
    -> name: yolov7app -> create repo -> copy URI
  > Now we create an EC2 Ubutnu machine -> name: safety-machine -> 
    OS: Ubuntu 20.04 (go for GPU machine in prod) -> t2.large 8GB -> create new key-pair
    -> name: yolov7-key -> rsa -> .pem -> create -> select new key-pair -> allow all traffic
    -> config storage: 32GB -> Launch -> Go to Instance id -> Connect (Coz we need to install docker)
    -> perform below commands in sequence in your EC2 machine:

      #optinal
      sudo apt-get update -y 
      sudo apt-get upgrade

      #required
      curl -fsSL https://get.docker.com -o get-docker.sh 
      sudo sh get-docker.sh
      sudo usermod -aG docker ubuntu 
      newgrp docker
  > Now we need to configure a self hosted runner on EC2 instance:
    Go to Github project settings -> Actions -> Runners -> Add new self-hosted runner
    -> choose OS (Linux) -> Run all the given command on your EC2 instance (if asked name: self-hosted)
    -> Once done, go back to github runners: state should be "idle"
  > Next is to add all secret variables to Github:
    -> Go to github project setting -> Secret and Variables -> Action 
    -> New Repository Secret for:
       AWS_ACCESS_KEY_ID =
       AWS_SECRET_ACCESS_KEY =
       AWS_REGION = us-east-1
       AWS_ECR_LOGIN_URI = 020866158197.dkr.ecr.us-east-1.amazonaws.com
       ECR_REPOSITORY_NAME = yolov7app
  > Now push your code to git so that CICD pipeline can run.




Workflow:
- constants
- config_entity
- artifact_entity
- components
- pipeline
- app.py
